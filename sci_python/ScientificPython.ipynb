{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('seaborn-bright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the data as a pandas DataFrame object\n",
    "data = pd.read_csv('iris_data.csv', index_col=0)\n",
    "help(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis with `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis ([EDA](https://en.wikipedia.org/wiki/Exploratory_data_analysis)) aims at characterizing properties of a data set often using visualization techniques to see \"what the data cen tell us\", perhaps even to formulate new hypotheses and guide subsequent experiments. EDA is also key to assess which statistical models are appropriate in the sense that their assumptions are not violated and to explore whether data transformations are necessary or to explore issues like [confounding](https://en.wikipedia.org/wiki/Confounding). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` provides plenty of methods of the `DataFrame` class for EDA, making it easy to get a quick overview of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw earlier, it's also very easy to produce simple plots directly from the dataframe. We will create some more detailed plots of the data, later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot('Sepal.Length','Sepal.Width',kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting/filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get True/False values for data matching specified criteria \n",
    "data['Species']=='setosa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use these boolean vectors to filter the whole dataset\n",
    "swidth_filter = data['Sepal.Width'] < 3.0\n",
    "thin_sepals = data[swidth_filter]\n",
    "print(thin_sepals.shape)\n",
    "thin_sepals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get subsets of the data, for each species\n",
    "seto = data[data['Species']=='setosa']\n",
    "vers = data[data['Species']=='versicolor']\n",
    "virg = data[data['Species']=='virginica']\n",
    "print(seto.shape, vers.shape, virg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grouping data based on the values in a column can be achieved more easily with the group_by() method\n",
    "data_by_species = data.groupby('Species')\n",
    "for grp, grp_data in data_by_species:\n",
    "    print(grp, grp_data.shape)\n",
    "    print(grp_data.describe())\n",
    "data_by_species.get_group('versicolor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use the .loc() and .iloc() methods to specify coordinates of data that you want to subset\n",
    "data.loc[5:10,'Species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you can operate on a whole series very quickly and easily\n",
    "seto_pl = seto['Petal.Length']\n",
    "print(seto_pl.head())\n",
    "halved = seto_pl / 2\n",
    "print(halved.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seto_pl = seto[seto['Petal.Length'] < 2.5].loc[10:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More data summary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for grp, grp_data in data.groupby('Species'):\n",
    "    print(grp)\n",
    "    print(grp_data['Petal.Width'].value_counts())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "box = data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do _so_ much more with `pandas` than we have the time to cover here. Check out the [documentation](http://pandas.pydata.org/pandas-docs/stable/index.html) to learn more about what's possible with this package, and see the more in-depth plotting examples below to get an idea of the approaches that you can take to visualise your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More detailed exploration of differences between species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is inspired by http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html by Gaël Varoquaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# separate the attribute columns from the class column (species)\n",
    "X = data[['Sepal.Length',  'Sepal.Width',  'Petal.Length',  'Petal.Width']]\n",
    "print(X.head())\n",
    "\n",
    "# map string species names to numeric codes for classification\n",
    "species_to_number = {'setosa': 0, \n",
    "                     'versicolor': 1, \n",
    "                     'virginica': 2}\n",
    "y = np.array([ species_to_number[spec] for spec in data['Species'] ])\n",
    "print(y)\n",
    "classes = np.unique(y)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# figure out plot coordinates\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the first two attributes of the iris data colored by class\n",
    "plt.scatter(X['Sepal.Length'], X['Sepal.Width'], s=40, c=y, cmap=plt.cm.viridis)\n",
    "plt.title('Two attributes of the Iris data', fontsize=14)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Explore separation of classes by individual attributes using boxplots\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plotdata = [X['Sepal.Length'][y == c] for c in classes]\n",
    "bx = plt.boxplot(plotdata, notch=True, patch_artist=True)\n",
    "#for p in bx['boxes']:\n",
    "#    p.set_facecolor('lightgray')\n",
    "#    p.set_edgecolor('black')\n",
    "#for p in bx['whiskers']:\n",
    "#    p.set_color('black')\n",
    "#    p.set_linestyle('solid')    \n",
    "\n",
    "plt.title('Attribute 0 (sepal length) broken down by class', fontsize=14)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Sepal length')\n",
    "\n",
    "# Overplot the means\n",
    "plt.plot(classes+1, [np.average(d) for d in plotdata], '*',\n",
    "             color='white', markeredgecolor='red', markersize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate how different attributes are between classes more formally using statistical tests\n",
    "In this section we demonstrate the application of several statistical tests (many more can be found in the scipy.stats package). Statistical tests are of course most useful, if we are not only interested in exploring the properties of our data, but also assess the statistical significane of (biologically) meaningful comparisons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon, kstest, f_oneway\n",
    "\n",
    "# Apply Wilcoxon test to assess statistical significance of\n",
    "# class-specific differences in sepal length (attribute 0)\n",
    "z_statistic, wilcox_p_value = wilcoxon(X['Sepal.Length'], y)\n",
    "print('Wilcoxon test P-value: %.3g'%wilcox_p_value)\n",
    "\n",
    "# Let's also try a parameteric test, e.g. ANOVA \n",
    "f_statistic, anova_p_value = f_oneway(X['Sepal.Length'][y==0], X['Sepal.Length'][y==1], X['Sepal.Length'][y==2])\n",
    "# f_statistic, anova_p_value = f_oneway(*[ X['Sepal.Length'][y==k] for k in set(y) ]) # alternative approach (don't miss out the * !)\n",
    "print('ANOVA P-value: %.3g'%anova_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But wait a second: are the assumptions of the parametric ANOVA test met?\n",
    "# Let's check with a Kolmogorov-Smirnov goodness-of-fit test whether\n",
    "# the attribute values follow a Gaussian distribution (assumed by ANOVA).\n",
    "fig, axarr = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for i in range(4):\n",
    "    axarr[i].hist(X[X.columns[i]], 20, normed=1, facecolor='green', alpha=0.75)\n",
    "    axarr[i].set_title('Histogram of %s' % X.columns[i], fontsize=14)\n",
    "    d_statistic, ks_p_value = kstest(X[X.columns[i]], 'norm')\n",
    "    axarr[i].text(min(X[X.columns[i]])+1, 0.2, 'K-S test P-value: %.3g'%ks_p_value,\n",
    "         fontsize=14, ha='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sections above we investigated some properties of the Iris data to assess which statistical analysis approaches may be suitable. Histograms and the [Kolmogorov-Smirnov goodness-of-fit test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) suggest that the attributes of the Iris data set do __not__ follow a Gaussian distribution, so we should be cautious with parametric statistical tests or models (such as [Student's t-test](https://en.wikipedia.org/wiki/Student%27s_t-test) or [ANOVA](https://en.wikipedia.org/wiki/Analysis_of_variance)) and rather apply non-parametric methods (such as the [Wilcoxon](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) or [Kruskal-Wallis tests](https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance)) that do not make distributional assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see whether we can cluster the Iris data set based on the first two attributes\n",
    "For demonstration, we will use [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering) here, but there is many alternative approaches. A nice overview on how to quickly explore other algorithms available from scikit-learn can be found here: http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html, this also provides a framework for visual evaluation of these algorithm.\n",
    "Some example code was borrowed from http://stamfordresearch.com/k-means-clustering-in-python/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize, fit and predict with the K-Means clustering algorithm\n",
    "np.random.seed(2016)\n",
    "\n",
    "num_clust = 3\n",
    "kmm = cluster.KMeans(n_clusters=num_clust)\n",
    "\n",
    "# Let's only look at the first two attributes for a first demonstration\n",
    "X_2d = X.iloc[:, :2]  \n",
    "kmm.fit(X_2d)\n",
    "y_pred = kmm.predict(X_2d)\n",
    "\n",
    "# As the unsupervised K-means algorithm is unaware of the order of classes in Y,\n",
    "# we need to (depending on random seed) re-arrange them to match class assignments\n",
    "# between y and y_pred \n",
    "print(y_pred)\n",
    "print(y)\n",
    "y_pred = np.choose(kmm.labels_, [1, 2, 0]).astype(np.int64)\n",
    "print(y_pred)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the clustering results\n",
    "\n",
    "# To be able to re-use the plotting code, we encapsulate it in a function\n",
    "def viz_clustering(ax, x_1, x_2, y, y_pred, centers=None):\n",
    "    ax.scatter(x_1, x_2, s=40, c=y_pred, cmap=plt.cm.viridis)\n",
    "    # Show cluster centers (means) if given\n",
    "    if centers is not None:\n",
    "        ax.scatter(centers[:, 0], centers[:, 1], linewidths=3, marker='x', s=300, color='black')\n",
    "\n",
    "    # Highlight wrong cluster assignments\n",
    "    ax.scatter(x_1[y != y_pred], x_2[y != y_pred], marker='o', s=70, \n",
    "               facecolors='none', edgecolors='red')\n",
    "\n",
    "    # Evaluate prediction accuracy and annotate plot\n",
    "    # Note that sklearn.metrics provides many alternative evaluation metrics\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    ax.text(max(x_1), max(x_2), 'Accuracy: '+str('%.3f'%acc), fontsize=14, ha='right')\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plt.gca()\n",
    "\n",
    "viz_clustering(ax, X['Sepal.Length'], X['Sepal.Width'], y, y_pred, centers=kmm.cluster_centers_[:,0:2])\n",
    "plt.title('K-means clustering of the Iris data (first 2 attributes)', fontsize=14)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's repeat the exercise with all four available attributes.\n",
    "np.random.seed(2016)\n",
    "kmm.fit(X)\n",
    "y_pred = kmm.predict(X)\n",
    "y_pred = np.choose(kmm.labels_, [2, 0, 1]).astype(np.int64)\n",
    "\n",
    "fig, axarr = plt.subplots(1, 3, figsize=(20, 6))\n",
    "plt_cnt = 0\n",
    "for i in classes[:-1]:\n",
    "    for j in classes[classes > i]:\n",
    "        #print(' i =',i,', j =',j)\n",
    "        viz_clustering(axarr[plt_cnt], X[X.columns[i]], X[X.columns[j]], y, y_pred, \n",
    "                       centers=kmm.cluster_centers_[:,[i,j]])\n",
    "        axarr[plt_cnt].set_title('K-means clustering of the whole Iris data', fontsize=14)\n",
    "        axarr[plt_cnt].set_xlabel(X.columns[i])\n",
    "        axarr[plt_cnt].set_ylabel(X.columns[j])\n",
    "        plt_cnt += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) as a means of dimensionality reduction\n",
    "In very rough terms this will rotate (i.e. orthogonally transform) a high-dimensional data set so that the first principal components (PCs) will correspond to the largest sources of variance in the original data. Dimensionality reduction is typically achieved by projection onto the first PCs; in the example below, we only consider the first two PCs.\n",
    "The code is again inspired by http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n",
    "by Gaël Varoquaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We use PCA to project from the whole data set \n",
    "# onto the first two PCs to better visualize cluster separation\n",
    "X_red = PCA(n_components=2).fit_transform(X) # returns a numpy.array object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the first two principal components\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.scatter(X_red[:, 0], X_red[:, 1], s=40, c=y, cmap=plt.cm.viridis)\n",
    "plt.title('PCA applied to the Iris data', fontsize=14)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's (formally) test the separation between classes in the PCA space \n",
    "by application of k-means clustering to the first tow PCs and comparison of clustering accuracy to the approach above where k-means used the first two are all attributes of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Repeat the clustering on the result of the PCA\n",
    "np.random.seed(2016)\n",
    "\n",
    "# Verify that the PCA projection is indeed into a 2D space\n",
    "print('Rank of PCA projection:',X_red.shape[1])\n",
    "\n",
    "kmm.fit(X_red) # fit the clustering on the first two PCs\n",
    "y_pred = kmm.predict(X_red)\n",
    "\n",
    "# Again, match class assignments between y and y_pred \n",
    "#print(y_pred)\n",
    "#print(y)\n",
    "y_pred = np.choose(kmm.labels_, [2, 0, 1]).astype(np.int64)\n",
    "#print(y_pred)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the clustering result (exactly as done above)\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plt.gca()\n",
    "\n",
    "viz_clustering(ax, X_red[:, 0], X_red[:, 1], y, y_pred, centers=kmm.cluster_centers_[:,0:2])\n",
    "plt.title('K-means clustering applied to the first two PCs of the Iris data')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "source": [
    "### Build a statistical classification model that recognizes the classes from the attributes\n",
    "This section is based on a more exhaustive comparison of classification algorithms from the [scikit-learn documentation](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) by Gaël Varoquaux and Andreas Müller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# As most training algorihtms have a stochastic component, setting the seed \n",
    "# of the random number generater is necessary for reproducibility\n",
    "np.random.seed(2016)\n",
    "\n",
    "# We will play with 4 rather popular classification algorihtms here\n",
    "names = ['Nearest Neighbors', 'Linear SVM', 'Naive Bayes', 'Random Forest']\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(5),\n",
    "    SVC(kernel='linear', C=1),\n",
    "    GaussianNB(),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "]\n",
    "# note that these classifier initializations also set hyperparameters \n",
    "# which in real analysis scenarios need to be tuned using model selection\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    #ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print('%20s test accuracy: %.3f'%(name, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For a visualization of classification results repeat the exercise\n",
    "np.random.seed(2016)\n",
    "\n",
    "# To be able to plot in 2 dimensions, restrict the data \n",
    "# to the attributes 1 and 2 and to classes 0 and 1\n",
    "idx_attr = ['Sepal.Width', 'Petal.Length']\n",
    "idx_exmp = y != 0\n",
    "X_2d = X[idx_attr]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2d[idx_exmp], \n",
    "                                                    y[idx_exmp], test_size=.4)\n",
    "\n",
    "# Construct a mesh for visualization of prediction scores\n",
    "x_min, x_max = X_2d[idx_attr[0]].min() - 0.5, X_2d[idx_attr[0]].max() + 0.5\n",
    "y_min, y_max = X_2d[idx_attr[1]].min() - 0.5, X_2d[idx_attr[1]].max() + 0.5\n",
    "h = 0.02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Prepare the plots (and color scheme)\n",
    "fig = plt.figure(figsize=(27, 9))\n",
    "plot_cnt = 1\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "# Loop over classification algorithms and \n",
    "# visualize their decision boundary in a subplot for each\n",
    "for name, clf in zip(names, classifiers):\n",
    "    ax = plt.subplot(1, len(classifiers) + 1, plot_cnt)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "    # Visualize the predictions on the mesh as a color contour\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "    # Plot the training points on top\n",
    "    ax.scatter(X_train[idx_attr[0]], X_train[idx_attr[1]], s=70, c=y_train, cmap=cm_bright, edgecolors='gray')\n",
    "    # and testing points with white edges\n",
    "    ax.scatter(X_test[idx_attr[0]], X_test[idx_attr[1]], s=70, c=y_test, cmap=cm_bright, edgecolors='white')\n",
    "\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_title(name)\n",
    "\n",
    "    # Add a text label with the prediction accuracy on the test set\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    ax.text(xx.max() - .3, yy.min() + .3, ('Test acc. = %.3f' % acc).lstrip('0'),\n",
    "            size=15, horizontalalignment='right')\n",
    "    plot_cnt += 1\n",
    "\n",
    "fig.subplots_adjust(left=.02, right=.98)\n",
    "\n",
    "plt.savefig('myplot.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [`pandas` documentation & tutorials](http://pandas.pydata.org/pandas-docs/stable/index.html)\n",
    "- [scikit-learn documentation & tutorials](http://scikit-learn.org/stable/)\n",
    "- [`numpy` for R users](http://mathesaurus.sourceforge.net/r-numpy.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
